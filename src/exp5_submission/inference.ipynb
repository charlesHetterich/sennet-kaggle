{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153186/634028945.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Union\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class SweepCube(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data: torch.Tensor, \n",
    "            patch_size: tuple[int, int, int],\n",
    "            stride: Optional[tuple[int, int, int]] = None\n",
    "        ):\n",
    "        assert data.ndim == 4, \"Data must be 4-dimensional\"\n",
    "\n",
    "        self.data = data\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride if stride is not None else patch_size\n",
    "\n",
    "        self.patches = (\n",
    "            (self.data.shape[1] - self.patch_size[0]) // self.stride[0] + 1,\n",
    "            (self.data.shape[2] - self.patch_size[1]) // self.stride[1] + 1,\n",
    "            (self.data.shape[3] - self.patch_size[2]) // self.stride[2] + 1,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.patches[0] * self.patches[1] * self.patches[2])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y, z = np.unravel_index(i, self.patches)\n",
    "        x, y, z = x * self.stride[0], y * self.stride[1], z * self.stride[2]\n",
    "\n",
    "        return self.data[\n",
    "            :,\n",
    "            x:x+self.patch_size[0],\n",
    "            y:y+self.patch_size[1],\n",
    "            z:z+self.patch_size[2],\n",
    "        ], torch.tensor([x, y, z])\n",
    "\n",
    "class Bayes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._registered_bayesian_modules: list[\"Bayes\"] = []\n",
    "\n",
    "    def register_bayes(self, module: Union[list[nn.Module], nn.Module]):\n",
    "        if 'Bayes' in [b.__name__ for b in type(module).__bases__]:\n",
    "            self._registered_bayesian_modules.append(module)\n",
    "        else:\n",
    "            # check if iterable\n",
    "            try:\n",
    "                for u in module: self.register_bayes(u)\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "    def penalty(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def penalize(self, alpha: float = 1e-3):\n",
    "        \"\"\"\n",
    "        alpha: the penalty coefficient\n",
    "        \"\"\"\n",
    "        return sum(u.penalize(alpha) for u in self._registered_bayesian_modules)\n",
    "\n",
    "    def decay_var(self, gamma: float = 0.5):\n",
    "        \"\"\"\n",
    "        gamma: variance decay rate\n",
    "        \"\"\"\n",
    "        for u in self._registered_bayesian_modules: u.decay_var(gamma)\n",
    "    \n",
    "    def rebase(self):\n",
    "        \"\"\"\n",
    "        Rebase the parameters to the current mean\n",
    "        \"\"\"\n",
    "        for u in self._registered_bayesian_modules: u.rebase()\n",
    "\n",
    "class Conv2DNormed(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer(\n",
    "            'norm_term',\n",
    "            torch.tensor(np.sqrt(\n",
    "                np.max([self.in_channels, self.out_channels])\n",
    "            ))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return super().forward(x) / self.norm_term\n",
    "\n",
    "class ConvBlock(Bayes):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_f: int,\n",
    "            out_f: int,\n",
    "            kernel_size: int = 3,\n",
    "\n",
    "            conv: nn.Module = nn.Conv2d,\n",
    "            activation: nn.Module = nn.GELU,\n",
    "            norm_fn: Optional[nn.Module] = None,\n",
    "            dropout: Optional[tuple[nn.Module, float]] = None,\n",
    "\n",
    "            padding: int = 1,\n",
    "            stride: int = 1,\n",
    "            block_depth: int = 4,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.out_f = out_f\n",
    "        def build_block(in_f, out_f, stride=1):\n",
    "            L = []\n",
    "            if dropout is not None:\n",
    "                L.append(dropout[0](dropout[1]))\n",
    "            L.append(conv(in_f, out_f, kernel_size, padding=padding, stride=stride, **kwargs))\n",
    "            if norm_fn is not None:\n",
    "                L.append(norm_fn(out_f))\n",
    "            L.append(activation())\n",
    "            return nn.Sequential(*L)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            build_block(\n",
    "                in_f if i == 0 else out_f,\n",
    "                out_f,\n",
    "                stride=stride if i == 0 else 1\n",
    "                ) for i in range(block_depth)\n",
    "        ])\n",
    "        for l in self.layers: self.register_bayes(l)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        zz = self.layers[0](x)\n",
    "        z = zz\n",
    "        for l in self.layers[1:]:\n",
    "            z = l(z) + z # Residual connection\n",
    "        return z + zz # Jump connection\n",
    "\n",
    "class UNetCrossBlock(Bayes):\n",
    "    def __init__(\n",
    "            self,\n",
    "            layers: list[int],\n",
    "            into_stage: int,\n",
    "\n",
    "            block_depth: int = 1,\n",
    "            connect_depth: int = 64,\n",
    "\n",
    "            conv: nn.Module = nn.Conv3d,\n",
    "            activation: nn.Module = nn.GELU,\n",
    "            pool_fn: nn.Module = nn.MaxPool3d,\n",
    "            resize_kernel: tuple = (2, 2, 2),\n",
    "            upsample_mode: str = 'trilinear',\n",
    "            norm_fn: Optional[nn.Module] = None,\n",
    "            dropout: Optional[tuple[nn.Module, float]] = None,\n",
    "\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert 0 <= into_stage < len(layers), f\"into_layer must be in [0, {len(layers)})\"\n",
    "        self.num_stages = len(layers)\n",
    "\n",
    "        self.resize_blocks = nn.ModuleList([])\n",
    "        for i in range(len(layers)):\n",
    "            block = nn.Sequential()                \n",
    "            if i < into_stage:\n",
    "                block.append(pool_fn((*(k ** (into_stage - i) for k in resize_kernel),), ceil_mode=True))\n",
    "            if i > into_stage:\n",
    "                block.append(nn.Upsample(scale_factor=(*(k ** (i - into_stage) for k in resize_kernel),), mode=upsample_mode))\n",
    "            block.append(\n",
    "                ConvBlock(\n",
    "                    connect_depth * self.num_stages if i > into_stage and i != self.num_stages - 1 else layers[i],\n",
    "                    connect_depth,\n",
    "                    conv=conv,\n",
    "                    activation=activation,\n",
    "                    block_depth=block_depth,\n",
    "                    dropout=dropout,\n",
    "                    norm_fn=norm_fn,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "            self.resize_blocks.append(block)\n",
    "        self.full_conv_block = ConvBlock(\n",
    "            connect_depth * len(layers),\n",
    "            connect_depth * len(layers),\n",
    "            conv=conv,\n",
    "            activation=activation,\n",
    "            block_depth=block_depth,\n",
    "            dropout=dropout,\n",
    "            norm_fn=norm_fn,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        for l in self.resize_blocks: self.register_bayes(l)\n",
    "        self.register_bayes(self.full_conv_block)\n",
    "\n",
    "\n",
    "    def forward(self, xs: list[torch.Tensor]):\n",
    "        assert len(xs) == self.num_stages, f\"expected input to be of length {self.num_stages}, but got {len(xs)}\"\n",
    "        zs = []\n",
    "        for i in range(len(xs)):\n",
    "            zs.append(self.resize_blocks[i](xs[i]))\n",
    "        return self.full_conv_block(torch.cat(zs, dim=1))\n",
    "\n",
    "# Architecture based on : https://arxiv.org/abs/2004.08790\n",
    "class UNet3P(Bayes):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_f: int = 1,\n",
    "            layers: list[int] = [64, 128, 256, 512, 1024],\n",
    "            out_f: int = 1,\n",
    "\n",
    "            block_depth: int = 4,\n",
    "            connect_depth: int = 64,\n",
    "\n",
    "            conv: nn.Module = nn.Conv3d,\n",
    "            activation: nn.Module = nn.GELU,\n",
    "            pool_fn: nn.Module = nn.MaxPool3d,\n",
    "            resize_kernel: tuple = (2, 2, 2),\n",
    "            upsample_mode: str = 'trilinear',\n",
    "            norm_fn: nn.Module = None,\n",
    "            dropout: tuple[nn.Module, float] = None,\n",
    "            \n",
    "            input_noise: Optional[float] = 0.1,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if input_noise is not None:\n",
    "            self.register_buffer('input_noise', torch.tensor(input_noise))\n",
    "        c = in_f\n",
    "\n",
    "        self.input_norm = norm_fn(in_f)\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "        for i, l in enumerate(layers):\n",
    "            block = nn.Sequential()\n",
    "            if i != 0:\n",
    "                block.append(pool_fn(resize_kernel))\n",
    "            block.append(\n",
    "                ConvBlock(\n",
    "                    c,\n",
    "                    l,\n",
    "                    conv=conv,\n",
    "                    activation=activation,\n",
    "                    block_depth=block_depth,\n",
    "                    dropout=dropout,\n",
    "                    norm_fn=norm_fn,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "            self.down_blocks.append(block)\n",
    "            c = l\n",
    "        \n",
    "        self.cross_blocks = nn.ModuleList([])\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.cross_blocks.append(\n",
    "                UNetCrossBlock(\n",
    "                    layers,\n",
    "                    i,\n",
    "                    block_depth=block_depth,\n",
    "                    connect_depth=connect_depth,\n",
    "                    conv=conv,\n",
    "                    activation=activation,\n",
    "                    pool_fn=pool_fn,\n",
    "                    resize_kernel=resize_kernel,\n",
    "                    upsample_mode=upsample_mode,\n",
    "                    norm_fn=norm_fn,\n",
    "                    dropout=dropout,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "    \n",
    "        self.out_blocks = nn.ModuleList([])\n",
    "        for i in reversed(range(len(layers) - 1)):\n",
    "            L = []\n",
    "            if i == 0:\n",
    "                L.append(\n",
    "                    ConvBlock(\n",
    "                        connect_depth * len(layers),\n",
    "                        connect_depth * len(layers),\n",
    "                        1,\n",
    "                        conv=conv,\n",
    "                        padding=0,\n",
    "                        activation=activation,\n",
    "                        block_depth=block_depth,\n",
    "                        dropout=dropout,\n",
    "                        norm_fn=norm_fn,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                )\n",
    "            L.append(\n",
    "                conv(\n",
    "                    connect_depth * len(layers),\n",
    "                    out_f,\n",
    "                    1,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "            self.out_blocks.append(nn.Sequential(*L))\n",
    "\n",
    "        self.mask_blocks = nn.ModuleList([\n",
    "            pool_fn((*(k ** (i + 1) for k in resize_kernel),))\n",
    "            for i in reversed(range(len(layers) - 2))\n",
    "        ])\n",
    "\n",
    "        for l in self.down_blocks: self.register_bayes(l)\n",
    "        for l in self.cross_blocks: self.register_bayes(l)\n",
    "        for l in self.out_blocks: self.register_bayes(l)\n",
    "\n",
    "    def _prep_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.input_norm(x)\n",
    "        if hasattr(self, 'input_noise') and self.training:\n",
    "            z += torch.randn_like(z) * self.input_noise\n",
    "        return z\n",
    "\n",
    "    def _down_fwd(self, x: torch.Tensor) -> list[torch.Tensor]:\n",
    "        down_agg = []\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            x = self.down_blocks[i](x)\n",
    "            down_agg.append(x)\n",
    "        return down_agg\n",
    "\n",
    "    def forward(self, x: torch.Tensor, up_depth: Optional[int] = None) -> list[torch.Tensor]:\n",
    "        up_depth = len(self.cross_blocks) if up_depth is None else up_depth\n",
    "        assert up_depth <= len(self.cross_blocks), f\"up_depth must be in [0, {len(self.cross_blocks)}]\"\n",
    "\n",
    "        z = self._prep_input(x)\n",
    "        down_agg = self._down_fwd(z)\n",
    "        \n",
    "        up_agg = []\n",
    "        for i in list(reversed(range(len(self.cross_blocks))))[:up_depth]:\n",
    "            cross_input = down_agg[:i + 1] + list(reversed([u for u in up_agg])) + [down_agg[-1]]\n",
    "            up_agg.append(self.cross_blocks[i](cross_input))\n",
    "\n",
    "        return [\n",
    "            o(x)\n",
    "            # for o, x in zip(self.out_blocks, [down_agg[-1]] + up_agg)\n",
    "            for o, x in zip(self.out_blocks[:up_depth], up_agg)\n",
    "        ]\n",
    "\n",
    "    def deep_masks(self, y: torch.Tensor) -> list[torch.Tensor]:\n",
    "        masks = []\n",
    "        for i in range(len(self.mask_blocks)):\n",
    "            masks.append(self.mask_blocks[i](y))\n",
    "        return masks + [y]\n",
    "\n",
    "class Patch2p5D(nn.Module):\n",
    "    def __init__(self, model: UNet3P):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4])\n",
    "        return self.model(z)[-1]\n",
    "\n",
    "\n",
    "class ScanInference2p5D(nn.Module):\n",
    "    def __init__(self, patch_fn, batch_size: int, quick: bool = False):\n",
    "        super().__init__()\n",
    "        self.patch_fn = patch_fn\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.patch_size = (8, 256, 256)\n",
    "        self.perms = torch.Tensor([\n",
    "            (0, 1, 2),\n",
    "            (0, 2, 1),\n",
    "            (1, 0, 2),\n",
    "            (1, 2, 0),\n",
    "            (2, 0, 1),\n",
    "            #(2, 1, 0), 5 perms to make self.pass_max a nice number\n",
    "        ]).int()\n",
    "        if quick:\n",
    "            self.perms = self.perms[:1]\n",
    "        \n",
    "        self.register_buffer('pass_max', torch.tensor(255 / len(self.perms)))\n",
    "    \n",
    "    def _forward(self, scan: torch.Tensor, device: torch.device = 'cpu') -> torch.Tensor:\n",
    "        agg_pred = torch.zeros_like(scan)\n",
    "        scan_loader = DataLoader(\n",
    "            SweepCube(scan, self.patch_size, stride=(1, self.patch_size[1], self.patch_size[2])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        for x, positions in scan_loader:\n",
    "            x = x.to(device).float()\n",
    "            pred = F.sigmoid(self.patch_fn(x)).cpu()\n",
    "            for p, pos in zip(pred, positions):\n",
    "                agg_pred[\n",
    "                    :,\n",
    "                    pos[0] + self.patch_size[0]//2,\n",
    "                    pos[1]:pos[1] + self.patch_size[1],\n",
    "                    pos[2]:pos[2] + self.patch_size[2],\n",
    "                ] += ((p.squeeze(1) * self.pass_max).round()).byte()#(p.squeeze(1) * 255).int\n",
    "        \n",
    "        return agg_pred\n",
    "\n",
    "    def forward(self, scan: torch.Tensor, device: torch.device = 'cpu') -> torch.Tensor:\n",
    "        agg_pred = torch.zeros_like(scan)\n",
    "\n",
    "        for perm in self.perms:\n",
    "            out = self._forward(scan.permute(0, *perm+1), device).permute(0, *torch.argsort(perm)+1)\n",
    "            agg_pred += out\n",
    "            del out\n",
    "        return agg_pred\n",
    "\n",
    "def rle_encode(mask):\n",
    "    pixel = mask.flatten()\n",
    "    pixel = np.concatenate([[0], pixel, [0]])\n",
    "    run = np.where(pixel[1:] != pixel[:-1])[0] + 1\n",
    "    run[1::2] -= run[::2]\n",
    "    rle = ' '.join(str(r) for r in run)\n",
    "    if rle == '':\n",
    "        rle = '1 0'\n",
    "    return rle\n",
    "\n",
    "def id_from_pth(pth: str):\n",
    "    parts = pth.split(\"/\")[-3:]\n",
    "    parts.pop(1)\n",
    "    return \"_\".join(parts)[:-4]\n",
    "\n",
    "def proprocess(_scan: np.ndarray):\n",
    "    scan = _scan.astype(np.float32)\n",
    "    smin, smax = np.min(scan), np.max(scan)\n",
    "    scan = (255 * (scan - smin) / (smax - smin)).astype(np.uint8)\n",
    "    scan = 255 - scan\n",
    "    clahe = cv2.createCLAHE(clipLimit=40.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(scan)\n",
    "\n",
    "def load_slice(pth, preprocess_fn):\n",
    "    return torch.tensor(\n",
    "        proprocess(tiff.imread(pth))\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# data_dir = '/kaggle/input/blood-vessel-segmentation'\n",
    "# model_pth = \"/kaggle/input/model-weights/model.pt\"\n",
    "data_dir = '/root/data'\n",
    "model_pth = './model.pt'\n",
    "\n",
    "is_submit= True\n",
    "try:\n",
    "    is_submit = len(glob(f\"{data_dir}/test/kidney_5/images/*.tif\"))!=3\n",
    "except:\n",
    "    pass\n",
    "scan_folders = glob(data_dir + \"/*/\")\n",
    "if not is_submit:\n",
    "    scan_folders = [\n",
    "        f'{data_dir}/train/kidney_2/',\n",
    "        f'{data_dir}train/kidney_3_sparse/'\n",
    "    ]\n",
    "\n",
    "\n",
    "# TODO: DELETE THIS FOR REAL SUBMISSION\n",
    "is_submit = True\n",
    "scan_folders = [\n",
    "    f'{data_dir}/train/kidney_2/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = Patch2p5D(\n",
    "    UNet3P(\n",
    "        in_f=8,\n",
    "        layers=[16, 32, 32, 32, 64, 64, 64],\n",
    "        block_depth=4,\n",
    "        connect_depth=6,\n",
    "        conv=Conv2DNormed,\n",
    "        pool_fn=nn.MaxPool2d,\n",
    "        resize_kernel=(2,2),\n",
    "        upsample_mode='bilinear',\n",
    "        norm_fn=nn.BatchNorm2d,\n",
    "        dropout=(nn.Dropout2d, 0.1)\n",
    "    )\n",
    ").to(device)\n",
    "patcher.model.load_state_dict(torch.load(model_pth, map_location=device))\n",
    "patcher.requires_grad_(False)\n",
    "patcher.eval()\n",
    "\n",
    "inference = ScanInference2p5D(\n",
    "    patcher,\n",
    "    batch_size=64,\n",
    "    quick=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scan /root/data/train/kidney_2/\n",
      "doing inference...\n",
      "aggregating slice rle encodings...\n",
      "loading scan /root/datatrain/kidney_3_sparse/\n",
      "saving aggregate dataframe of rle encodings...\n"
     ]
    }
   ],
   "source": [
    "submission_list = []\n",
    "for scan_fn in scan_folders:\n",
    "    slices, ids = [], []\n",
    "    print(f\"loading scan {scan_fn}\")\n",
    "    for pth in sorted(glob(scan_fn + \"images/*.tif\")):\n",
    "        slices.append(load_slice(pth, proprocess))\n",
    "        ids.append(id_from_pth(pth))\n",
    "    if len(slices) == 0:\n",
    "        continue\n",
    "    scan = torch.stack(slices).unsqueeze(0)\n",
    "\n",
    "    print(\"doing inference...\")\n",
    "    pmask = inference(scan, device).squeeze(0) > (255 / 2)\n",
    "\n",
    "    print(\"aggregating slice rle encodings...\")\n",
    "    for id, smask in zip(ids, pmask):\n",
    "        submission_list.append(\n",
    "            pd.DataFrame(data={\n",
    "                'id'  : id,\n",
    "                'rle' : rle_encode(smask.numpy()),\n",
    "            },index=[0])\n",
    "        )\n",
    "    del scan\n",
    "    del pmask\n",
    "    del slices\n",
    "    del ids\n",
    "\n",
    "print(\"saving aggregate dataframe of rle encodings...\")\n",
    "submission_df = pd.concat(submission_list)\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('submission.csv')\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# slices = []\n",
    "# for row in submission_df.itertuples():\n",
    "#     slices.append()\n",
    "pmask2 = torch.stack(\n",
    "    [torch.tensor(rle_decode(row.rle, (1041, 1511)))\n",
    "    for row in submission_df.itertuples()]\n",
    ").unsqueeze(0).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import util as otil\n",
    "true_mask = torch.load('/root/data/cache/train/kidney_2/labels.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2217, 1041, 1511])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8748)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otil.DiceScore()(pmask2.bool(), true_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmask = out_mask > 0.5\n",
    "det_mask = torch.cat([ #collect TP, FP, FN\n",
    "    true_mask * pmask2, # true positive\n",
    "    (~true_mask) * pmask2, # false positive\n",
    "    true_mask * (~pmask2) # false negative\n",
    "], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11605204), tensor(639534), tensor(2683305))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_mask[0].sum(), det_mask[1].sum(), det_mask[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_mask.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_mask.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import ipywidgets as widgets\n",
    "\n",
    "class Display:\n",
    "    def __init__(self, scan: torch.Tensor = None, mask: torch.Tensor = None):\n",
    "        self.scan = scan\n",
    "        self.mask = mask\n",
    "\n",
    "    def _view_slice(self, i: int, slice_dim: int, ax: plt.Axes = None):\n",
    "        ax.set_facecolor('black')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        slice_idx = [slice(None), slice(None), slice(None)]\n",
    "        slice_idx[slice_dim] = i\n",
    "\n",
    "        if self.scan is not None:\n",
    "            ax.imshow(self.scan[0][tuple(slice_idx)], cmap='gray')\n",
    "        if self.mask is not None:\n",
    "            if self.mask.shape[0] == 1:\n",
    "                ax.imshow(self.mask[0][tuple(slice_idx)], cmap=mcolors.ListedColormap(['none', 'blue']), alpha=0.5)\n",
    "            else:\n",
    "                ax.imshow(self.mask[0][tuple(slice_idx)], cmap=mcolors.ListedColormap(['none', 'green']), alpha=0.5)\n",
    "                ax.imshow(self.mask[1][tuple(slice_idx)], cmap=mcolors.ListedColormap(['none', 'red']), alpha=0.5)\n",
    "                ax.imshow(self.mask[2][tuple(slice_idx)], cmap=mcolors.ListedColormap(['none', 'yellow']), alpha=0.5)\n",
    "\n",
    "    @staticmethod\n",
    "    def _view_slices(i: int, displays: list['Display'], slice_dim: int):\n",
    "        _, axs = plt.subplots(1, len(displays), figsize=(15, 15))\n",
    "        if len(displays) == 1:\n",
    "            axs = [axs]\n",
    "        for ax, display in zip(axs, displays):\n",
    "            display._view_slice(i, slice_dim, ax)\n",
    "\n",
    "    @staticmethod\n",
    "    def view(displays: list['Display'], slice_dim: int = 0):\n",
    "        slider_max = displays[0].scan.shape[slice_dim+1] - 1 if displays[0].scan is not None else displays[0].mask.shape[slice_dim+1] - 1\n",
    "        slider  = widgets.IntSlider(min=0, max=slider_max, step=1, value=0)\n",
    "        widgets.interact(Display._view_slices, i=slider, displays=widgets.fixed(displays), slice_dim=widgets.fixed(slice_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_mask.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defcf059b9a54293ba507712dbc7f579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=2216), Output()), _dom_classes=('widget-interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Display.view([Display(mask=det_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices = glob(scan_folders[0] + \"images/*.tif\")\n",
    "slices.sort()\n",
    "slices == glob(scan_folders[0] + \"images/*.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000.tif', '0001.tif']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [\"0001.tif\", \"0000.tif\"]\n",
    "x.sort()\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
