{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch as tc \n",
    "import torch.nn as nn  \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os,sys,cv2\n",
    "from torch.cuda.amp import autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import tifffile as tiff\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import util\n",
    "\n",
    "# import tensorrt as trt\n",
    "# import pycuda.driver as cuda\n",
    "# import pycuda.autoinit  # This is needed for initializing CUDA driver\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    model_name = 'Unet'\n",
    "    router_backbone = 'resnext50_32x4d'\n",
    "    backbones = [\"efficientnet-b0\"] * 25\n",
    "\n",
    "    input_size = 1024\n",
    "    exp_input_size = 128\n",
    "\n",
    "    batch = 2\n",
    "    exp_batch = 128\n",
    "    exp_batch_eval = 256\n",
    "\n",
    "class DiceScore(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.register_buffer('smooth', tc.tensor(smooth))\n",
    "\n",
    "    def forward(self, p_y: tc.Tensor, y: tc.Tensor, w: Optional[tc.Tensor] = None, mode: str = 'together') -> tc.Tensor:\n",
    "        \"\"\"\n",
    "        p_y: (B, ...) Tensor of probabilities\n",
    "        y: (B, ...) Tensor of binary labels\n",
    "        w: (B) Tensor of sample weightings\n",
    "        \"\"\"\n",
    "        \n",
    "        assert mode in ['together', 'separate'], \"Mode must be 'together' or 'separate'\"\n",
    "        flat_prob = p_y.view(p_y.shape[0], -1)\n",
    "        flat_y = y.view(y.shape[0], -1)\n",
    "\n",
    "        intersection = (flat_prob * flat_y).sum(1)\n",
    "        cardinality = flat_prob.sum(1) + flat_y.sum(1)\n",
    "        if w is not None:\n",
    "            intersection *= w\n",
    "            cardinality *= w\n",
    "        if mode == 'together':\n",
    "            return (2. * intersection.sum() + self.smooth) / (cardinality.sum() + self.smooth)\n",
    "        return ((2. * intersection + self.smooth) / (cardinality + self.smooth))\n",
    "\n",
    "        # if mode == 'together':\n",
    "        #     intersection = (flat_prob * flat_y).sum()\n",
    "        #     cardinality = flat_prob.sum() + flat_y.sum()\n",
    "        #     return (2. * intersection + self.smooth) / (cardinality + self.smooth)\n",
    "\n",
    "        # elif mode == 'separate':\n",
    "        #     intersection = (flat_prob * flat_y).sum(1)\n",
    "        #     cardinality = flat_prob.sum(1) + flat_y.sum(1)\n",
    "        #     return ((2. * intersection + self.smooth) / (cardinality + self.smooth))\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, in_f: int, out_f: int, CFG: CFG, weight=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = smp.Unet(\n",
    "            encoder_name=CFG.router_backbone, \n",
    "            encoder_weights=weight,\n",
    "            in_channels=in_f,\n",
    "            classes=out_f,\n",
    "            activation=None,\n",
    "        ).encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveMaxPool2d(CFG.input_size // CFG.exp_input_size),\n",
    "            nn.Conv2d(2048, len(CFG.backbones), kernel_size=1, stride=1, padding=0, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: tc.Tensor) -> tc.Tensor:\n",
    "        z = self.encoder(x)[-1]\n",
    "        z = self.classifier(z)\n",
    "        return z\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, backbone: str,  in_f: int, out_f: int, CFG: CFG, weight=None):\n",
    "        super().__init__()\n",
    "        self.batch = CFG.exp_batch, CFG.exp_batch_eval\n",
    "        self.net = smp.Unet(\n",
    "            encoder_name=backbone,\n",
    "            encoder_weights=weight,\n",
    "            in_channels=in_f,\n",
    "            classes=out_f,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: tc.Tensor) -> tc.Tensor:\n",
    "        batch = self.batch[0] if self.training else self.batch[1]\n",
    "        for i in range(0, len(x), batch):\n",
    "            y = self.net(x[i:i+batch])\n",
    "            if i == 0:\n",
    "                out = y\n",
    "            else:\n",
    "                out = tc.cat((out, y), 0)\n",
    "        return out\n",
    "\n",
    "class SegMoE(nn.Module):\n",
    "    def __init__(self, topk: int, CFG: CFG, loss_scaler: tc.cuda.amp.GradScaler, weight=None):\n",
    "        super().__init__()\n",
    "        assert 0 < topk <= len(CFG.backbones), f\"topk should be in (0, {len(CFG.backbones)}]\"\n",
    "        self.topk = topk\n",
    "        self.dice = DiceScore()\n",
    "        self.scaler=loss_scaler\n",
    "        self.router_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.router = Router(1,1, CFG, weight)\n",
    "        self.experts = nn.ModuleList([smp.Unet(\n",
    "            encoder_name=backbone, \n",
    "            encoder_weights=weight,\n",
    "            in_channels=1,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        ) for backbone in CFG.backbones])\n",
    "        self.register_buffer('n_experts', tc.tensor(len(self.experts), dtype=tc.float32))\n",
    "        self.grid_size = CFG.input_size // CFG.exp_input_size\n",
    "\n",
    "        self.expert_load_dist = tc.ones(len(self.experts), dtype=tc.float32).cuda()\n",
    "\n",
    "    def _to_patches(self, x: tc.Tensor) -> tc.Tensor:\n",
    "        return x \\\n",
    "            .unfold(2, CFG.exp_input_size, CFG.exp_input_size) \\\n",
    "            .unfold(3, CFG.exp_input_size, CFG.exp_input_size) \\\n",
    "            .reshape(-1, 1, CFG.exp_input_size, CFG.exp_input_size)\n",
    "\n",
    "    def _smooth_weight(self, expert_dist: tc.Tensor) -> tc.Tensor:\n",
    "        dist_balancer = (self.expert_load_dist.sum() / (self.expert_load_dist))\n",
    "        dist_balancer = self.n_experts * dist_balancer.softmax(dim=-1)\n",
    "        dist_balancer, dist_balancer.sum()\n",
    "        balanced_dist = (expert_dist * dist_balancer)\n",
    "        balanced_dist /= balanced_dist.sum(1, keepdim=True) # must sum to 1 on expert-dist axis\n",
    "        self.expert_load_dist += balanced_dist.sum(0)\n",
    "        self.expert_load_dist *= 0.9\n",
    "\n",
    "        return balanced_dist\n",
    "\n",
    "    def _assemble_patches(self, patches: tc.Tensor) -> tc.Tensor:\n",
    "        return patches \\\n",
    "            .view(-1, self.grid_size, self.grid_size, 1, CFG.exp_input_size, CFG.exp_input_size) \\\n",
    "            .permute(0, 3, 1, 4, 2, 5) \\\n",
    "            .reshape(-1, 1, CFG.input_size, CFG.input_size)\n",
    "\n",
    "\n",
    "    def _predict(self, x: tc.Tensor, dist: tc.Tensor) -> tc.Tensor:\n",
    "        batch_size, _num_classes = dist.size()\n",
    "        top_k_values, top_k_indices = tc.topk(dist, self.topk, dim=1)\n",
    "        batch_indices = tc.arange(batch_size).unsqueeze(1).expand(-1, self.topk)\n",
    "\n",
    "        k_hot_weights = tc.zeros_like(dist)\n",
    "        k_hot_outputs = tc.zeros_like(dist)\n",
    "        k_hot_outputs[batch_indices, top_k_indices] = 1\n",
    "        k_hot_weights[batch_indices, top_k_indices] = top_k_values / top_k_values.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        agg_pred = tc.zeros_like(x, dtype=tc.float32).squeeze(1)\n",
    "        for i in range(len(self.experts)):\n",
    "            idx = k_hot_outputs[:, i].bool()\n",
    "            if idx.sum() > 0:\n",
    "                agg_pred[idx] += self.experts[i](x[idx])[:, 0].sigmoid() * k_hot_weights[idx, i].unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        return self._assemble_patches(agg_pred)\n",
    "\n",
    "    def forward(self, image: tc.Tensor, label: Optional[tc.Tensor] = None, flat_dist: bool = False) -> (tc.Tensor, tc.Tensor):\n",
    "        # if self.training: assert label is not None, \"Expected label tensor in training mode.\"\n",
    "        # router_logits = self.router(image)\n",
    "        # preds_list = [expert(image)[:, 0].sigmoid() for expert in self.experts]\n",
    "        # return preds_list, router_logits\n",
    "\n",
    "        with autocast():\n",
    "            router_logits = self.router(image).permute(0, 2, 3, 1).reshape(-1, len(self.experts))\n",
    "            expert_dist = router_logits.detach().softmax(1)\n",
    "            expert_inputs = self._to_patches(image)\n",
    "\n",
    "            # Sparse predict if eval, else prep dist & labels for training\n",
    "            if not self.training: return self._predict(expert_inputs, expert_dist), expert_dist\n",
    "            assert label is not None, \"Expected label tensor in training mode.\"\n",
    "            expert_labels = self._to_patches(label)\n",
    "            expert_dist = self._smooth_weight(expert_dist) if not flat_dist \\\n",
    "                else tc.ones_like(expert_dist) / self.n_experts\n",
    "\n",
    "        experts_sample_dice = []\n",
    "        agg_pred = tc.zeros_like(expert_inputs, dtype=tc.float32)\n",
    "        for expert, w in zip(self.experts, expert_dist.unbind(1)): # every expert sees all examples\n",
    "            with autocast():\n",
    "                # w = w.flatten()\n",
    "                z = expert(expert_inputs).sigmoid()\n",
    "                expert_loss = (1 - self.dice(z, expert_labels, w))\n",
    "                sample_dice = self.dice(z.detach(), expert_labels, mode='separate')\n",
    "\n",
    "            self.scaler.scale(expert_loss).backward()\n",
    "            experts_sample_dice.append(sample_dice)\n",
    "            agg_pred += z * w.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # compute router loss\n",
    "        experts_sample_dice = tc.stack(experts_sample_dice, dim=1)\n",
    "        router_labels = tc.argmax(experts_sample_dice, dim=1)\n",
    "        # router_loss = self.router_loss(router_logits, router_labels)\n",
    "        router_loss = self.router_loss(router_logits, router_labels)\n",
    "        self.scaler.scale(router_loss).backward()\n",
    "\n",
    "        return self._assemble_patches(agg_pred), expert_dist\n",
    "\n",
    "        # expert_pred_list = [expert(expert_inputs).sigmoid() for expert in self.experts]\n",
    "        # combined_preds = tc.stack([\n",
    "        #     expert_pred_list[i] * router_logits.softmax(1)[:, i].flatten().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        #     for i in range(len(self.experts))\n",
    "        # ]).sum(0)\n",
    "\n",
    "        # seg_pred = agg_pred.view(-1, self.grid_size, self.grid_size, 1, CFG.exp_input_size, CFG.exp_input_size)\n",
    "        # seg_pred = seg_pred.permute(0, 3, 1, 4, 2, 5).reshape(-1, 1, CFG.input_size, CFG.input_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = tc.cuda.amp.GradScaler()\n",
    "model = SegMoE(1, CFG, scaler, \"imagenet\").cuda()\n",
    "model.train()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.expert_load_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x, y = tc.rand(batch_size, 1, CFG.input_size, CFG.input_size), tc.rand(batch_size, 1, CFG.input_size, CFG.input_size)\n",
    "x=x.cuda().to(tc.float32)\n",
    "y=y.cuda().to(tc.float32)\n",
    "# x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n",
    "# x=add_noise(x,max_randn_rate=0.5,x_already_normed=True)\n",
    "\n",
    "# with autocast():\n",
    "    # compute prediction\n",
    "pred = model(x, y)\n",
    "    # expert_dist = expert_dist_logits.detach().softmax(dim=-1)\n",
    "    # if step < explore_experts_until:\n",
    "    #     expert_dist = tc.ones_like(expert_dist) / model.n_experts\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
