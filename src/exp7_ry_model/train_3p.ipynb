{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc \n",
    "import torch.nn as nn  \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os,sys,cv2\n",
    "from torch.cuda.amp import autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DataParallel\n",
    "from glob import glob\n",
    "import tifffile as tiff\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import util\n",
    "\n",
    "log_board = util.diagnostics.LogBoard('log_dir', 6005)\n",
    "log_board.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_augm = 0.05 #0.5\n",
    "#add rotate.  less p_augm\n",
    "\n",
    "class CFG:\n",
    "    # ============== pred target =============\n",
    "    target_size = 1\n",
    "\n",
    "    # ============== model CFG =============\n",
    "    model_name = 'Unet'\n",
    "    backbone = 'resnext50_32x4d'\n",
    "\n",
    "    in_chans = 1   #5 # 65\n",
    "    # ============== training CFG =============\n",
    "    image_size = 256 # 512 # 512\n",
    "    input_size = 256 # 512 # 512\n",
    "\n",
    "    train_batch_size = 8 #4 #16\n",
    "    valid_batch_size = 8\n",
    "\n",
    "    epochs = 31 #30 #25\n",
    "    lr = 8e-3\n",
    "    chopping_percentile=1e-3\n",
    "    # ============== fold =============\n",
    "    valid_id = 1\n",
    "\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        A.Rotate(limit=270, p= 0.5),\n",
    "        A.RandomScale(scale_limit=(0.8,1.25),interpolation=cv2.INTER_CUBIC,p=p_augm),\n",
    "        A.RandomCrop(input_size, input_size,p=1),\n",
    "        A.RandomGamma(p=p_augm*2/3),\n",
    "        A.RandomBrightnessContrast(p=p_augm,),\n",
    "        A.GaussianBlur(p=p_augm),\n",
    "        A.MotionBlur(p=p_augm),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=p_augm),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    train_aug = A.Compose(train_aug_list)\n",
    "    valid_aug_list = [\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    valid_aug = A.Compose(valid_aug_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, CFG, weight=None):\n",
    "        super().__init__()\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=CFG.backbone, \n",
    "            encoder_weights=weight,\n",
    "            in_channels=CFG.in_chans,\n",
    "            classes=CFG.target_size,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        output = self.model(image)\n",
    "        # output = output.squeeze(-1)\n",
    "        return output[:,0]#.sigmoid()\n",
    "\n",
    "\n",
    "def build_model(weight=\"imagenet\"):\n",
    "    load_dotenv()\n",
    "\n",
    "    print('model_name', CFG.model_name)\n",
    "    print('backbone', CFG.backbone)\n",
    "\n",
    "    model = CustomModel(CFG, weight)\n",
    "\n",
    "    return model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_1024(img , image_size = 1024):\n",
    "    if image_size > img.shape[1]:\n",
    "       img = np.rot90(img)\n",
    "       start1 = (CFG.image_size - img.shape[0])//2 \n",
    "       top =     img[0                    : start1,   0: img.shape[1] ]\n",
    "       bottom  = img[img.shape[0] -start1 : img.shape[0],   0 : img.shape[1] ]\n",
    "       img_result = np.concatenate((top,img,bottom ),axis=0)\n",
    "       img_result = np.rot90(img_result)\n",
    "       img_result = np.rot90(img_result)\n",
    "       img_result = np.rot90(img_result)\n",
    "    else :\n",
    "       img_result = img\n",
    "    return img_result\n",
    "\n",
    "def to_1024_no_rot(img, image_size = 1024):\n",
    "    if image_size > img.shape[0]:  \n",
    "       start1 = ( image_size - img.shape[0])//2\n",
    "       top =     img[0                    : start1,   0: img.shape[1] ]\n",
    "       bottom  = img[img.shape[0] -start1 : img.shape[0],   0 : img.shape[1] ]\n",
    "       img_result = np.concatenate((top,img,bottom ),axis=0)\n",
    "    else: \n",
    "       img_result = img\n",
    "    return img_result\n",
    "\n",
    "#  add border\n",
    "def to_1024_1024(img  , image_size = 1024 ):\n",
    "     img_result = to_1024(img, image_size )\n",
    "     return img_result\n",
    "    \n",
    "#  drop border\n",
    "def to_original ( im_after, img, image_size = 1024 ):\n",
    "    top_ = 0\n",
    "    left_ = 0\n",
    "    if (im_after.shape[0] > img.shape[0]):\n",
    "             top_  = ( image_size - img.shape[0])//2 \n",
    "    if    (im_after.shape[1] > img.shape[1]) :\n",
    "             left_  = ( image_size - img.shape[1])//2  \n",
    "    if (top_>0)or (left_>0) :\n",
    "             img_result = im_after[top_                    : img.shape[0] + top_,   left_: img.shape[1] + left_ ]\n",
    "             #print(im_after.shape,'-->',img_result.shape)\n",
    "    else:\n",
    "             img_result = im_after\n",
    "    return img_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(x:tc.Tensor)->tc.Tensor:\n",
    "    \"\"\"input.shape=(batch,f1,...)\"\"\"\n",
    "    shape=x.shape\n",
    "    if x.ndim>2:\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "    \n",
    "    min_=x.min(dim=-1,keepdim=True)[0]\n",
    "    max_=x.max(dim=-1,keepdim=True)[0]\n",
    "    if min_.mean()==0 and max_.mean()==1:\n",
    "        return x.reshape(shape)\n",
    "    \n",
    "    x=(x-min_)/(max_-min_+1e-9)\n",
    "    return x.reshape(shape)\n",
    "\n",
    "def norm_with_clip(x:tc.Tensor,smooth=1e-5):\n",
    "    dim=list(range(1,x.ndim))\n",
    "    mean=x.mean(dim=dim,keepdim=True)\n",
    "    std=x.std(dim=dim,keepdim=True)\n",
    "    x=(x-mean)/(std+smooth)\n",
    "    x[x>5]=(x[x>5]-5)*1e-3 +5\n",
    "    x[x<-3]=(x[x<-3]+3)*1e-3-3\n",
    "    return x\n",
    "\n",
    "def add_noise(x:tc.Tensor,max_randn_rate=0.1,randn_rate=None,x_already_normed=False):\n",
    "    \"\"\"input.shape=(batch,f1,f2,...) output's var will be normalizate  \"\"\"\n",
    "    ndim=x.ndim-1\n",
    "    if x_already_normed:\n",
    "        x_std=tc.ones([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n",
    "        x_mean=tc.zeros([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n",
    "    else: \n",
    "        dim=list(range(1,x.ndim))\n",
    "        x_std=x.std(dim=dim,keepdim=True)\n",
    "        x_mean=x.mean(dim=dim,keepdim=True)\n",
    "    if randn_rate is None:\n",
    "        randn_rate=max_randn_rate*np.random.rand()*tc.rand(x_mean.shape,device=x.device,dtype=x.dtype)\n",
    "    cache=(x_std**2+(x_std*randn_rate)**2)**0.5\n",
    "    #https://blog.csdn.net/chaosir1991/article/details/106960408\n",
    "    \n",
    "    return (x-x_mean+tc.randn(size=x.shape,device=x.device,dtype=x.dtype)*randn_rate*x_std)/(cache+1e-7)\n",
    " \n",
    "class Data_loader(Dataset):\n",
    "     \n",
    "    def __init__(self,paths,is_label):\n",
    "        self.paths=paths\n",
    "        self.paths.sort()\n",
    "        self.is_label=is_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "         \n",
    "        img = cv2.imread(self.paths[index],cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        img = to_1024_1024(img , image_size = CFG.image_size ) #  to_original( im_after, img_save, image_size = 1024)\n",
    "\n",
    "        img = tc.from_numpy(img.copy())\n",
    "        if self.is_label:\n",
    "            img=(img!=0).to(tc.uint8)*255\n",
    "        else:\n",
    "            img=img.to(tc.uint8)\n",
    "        return img\n",
    "\n",
    "def load_data(paths,is_label=False):\n",
    "    data_loader=Data_loader(paths,is_label)\n",
    "    data_loader=DataLoader(data_loader, batch_size=8, num_workers=2)  \n",
    "    data=[]\n",
    "    for x in tqdm(data_loader):\n",
    "        data.append(x)\n",
    "    x=tc.cat(data,dim=0)\n",
    "    del data\n",
    "    if not is_label:\n",
    "        ########################################################################\n",
    "        TH=x.reshape(-1).numpy()\n",
    "        index = -int(len(TH) * CFG.chopping_percentile)\n",
    "        TH:int = np.partition(TH, index)[index]\n",
    "        x[x>TH]=int(TH)\n",
    "        ########################################################################\n",
    "        TH=x.reshape(-1).numpy()\n",
    "        index = -int(len(TH) * CFG.chopping_percentile)\n",
    "        TH:int = np.partition(TH, -index)[-index]\n",
    "        x[x<TH]=int(TH)\n",
    "        ########################################################################\n",
    "        x=(min_max_normalization(x.to(tc.float16)[None])[0]*255).to(tc.uint8)\n",
    "    return x\n",
    "\n",
    "\n",
    "#https://www.kaggle.com/code/kashiwaba/sennet-hoa-train-unet-simple-baseline\n",
    "def dice_coef(y_pred:tc.Tensor,y_true:tc.Tensor, thr=0.5, dim=(-1,-2), epsilon=0.001):\n",
    "    #y_pred=y_pred.sigmoid()\n",
    "    y_true = y_true.to(tc.float32)\n",
    "    y_pred = (y_pred>thr).to(tc.float32)\n",
    "    inter = (y_true*y_pred).sum(dim=dim)\n",
    "    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n",
    "    dice = ((2*inter+epsilon)/(den+epsilon)).mean()\n",
    "    return dice\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = inputs.sigmoid()   \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class SurfaceDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(SurfaceDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, surface_true, volume_true, smooth=1, k_size=3):\n",
    "        pred = pred.sigmoid()\n",
    "        surface_pred = pred * surface_true\n",
    "        volume_pred = pred * volume_true\n",
    "      \n",
    "        surface_pred = surface_pred.view(-1)\n",
    "        surface_true = surface_true.view(-1)\n",
    "        volume_pred = volume_pred.view(-1)\n",
    "        volume_true = volume_true.view(-1)\n",
    "        \n",
    "        surface_intersection = (surface_pred * surface_true).sum()\n",
    "        volume_intersection = (volume_pred * volume_true).sum()\n",
    "\n",
    "        surface_dice = (2. * (surface_intersection + (0.35*volume_intersection)) + smooth) / (surface_pred.sum() + surface_true.sum() + (0.35*volume_pred.sum()) + (0.35*volume_true.sum()) + smooth)\n",
    "\n",
    "        return 1 - surface_dice\n",
    "\n",
    "\n",
    "class Kaggld_Dataset(Dataset):\n",
    "    def __init__(self,x:list,y:list,arg=False):\n",
    "        super(Dataset,self).__init__()\n",
    "        self.x=x#list[(C,H,W),...]\n",
    "        self.y=y#list[(C,H,W),...]\n",
    "        self.image_size=CFG.image_size\n",
    "        self.in_chans=CFG.in_chans\n",
    "        self.arg=arg\n",
    "        if arg:\n",
    "            self.transform=CFG.train_aug\n",
    "        else: \n",
    "            self.transform=CFG.valid_aug\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return sum([y.shape[0]-self.in_chans for y in self.y])\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        i=0\n",
    "        for x in self.x:\n",
    "            if index>x.shape[0]-self.in_chans:\n",
    "                index-=x.shape[0]-self.in_chans\n",
    "                i+=1\n",
    "            else:\n",
    "                break\n",
    "        x=self.x[i]\n",
    "        y=self.y[i]\n",
    "        \n",
    "        print (f'x.shape[1] ={x.shape[1]}    x.shape[2]={x.shape[2]}')\n",
    "        \n",
    "        x_index= (x.shape[1]-self.image_size)//2 #np.random.randint(0,x.shape[1]-self.image_size)\n",
    "        y_index= (x.shape[2]-self.image_size)//2 # np.random.randint(0,x.shape[2]-self.image_size)\n",
    "        # i i+5 \n",
    "        x=x[index:index+self.in_chans   ,   x_index:x_index+self.image_size,   y_index:y_index+self.image_size]\n",
    "        # i+2\n",
    "        y=y[index+self.in_chans//2   ,      x_index:x_index+self.image_size,   y_index:y_index+self.image_size]\n",
    "\n",
    "        data = self.transform(image=x.numpy().transpose(1,2,0), mask=y.numpy())\n",
    "        x = data['image']\n",
    "        y = data['mask']>=127\n",
    "        if self.arg:\n",
    "            i=np.random.randint(4)\n",
    "            x=x.rot90(i,dims=(1,2))\n",
    "            y=y.rot90(i,dims=(0,1))\n",
    "            for i in range(3):\n",
    "                if np.random.randint(2):\n",
    "                    x=x.flip(dims=(i,))\n",
    "                    if i>=1:\n",
    "                        y=y.flip(dims=(i-1,))\n",
    "        return x,y#(uint8,uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 25/285 [00:00<00:04, 58.18it/s]TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      " 17%|█▋        | 49/285 [00:00<00:03, 67.74it/s]E0201 05:19:41.091455 139724285792896 program.py:298] TensorBoard could not bind to port 6005, it was already in use\n",
      "ERROR: TensorBoard could not bind to port 6005, it was already in use\n",
      "100%|██████████| 285/285 [00:04<00:00, 69.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2279, 1303, 912])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [00:04<00:00, 58.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2279, 1303, 912])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:02<00:00, 25.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([501, 1706, 1510])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:02<00:00, 23.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([501, 1706, 1510])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load training/valid data\n",
    "\n",
    "train_x=[]\n",
    "train_y=[]\n",
    "\n",
    "root_path=\"/root/data\"\n",
    "parhs=[\"/root/data/train/kidney_1_dense\"]\n",
    "for i,path in enumerate(parhs):\n",
    "    if path==\"/train/kidney_3_dense\":\n",
    "        continue\n",
    "    x=load_data(glob(f\"{path}/images/*\"),is_label=False)\n",
    "    print(x.shape)\n",
    "    y=load_data(glob(f\"{path}/labels/*\"),is_label=True)\n",
    "    print(y.shape)\n",
    "    train_x.append(x)\n",
    "    train_y.append(y)\n",
    "\n",
    "    #(C,H,W)\n",
    "\n",
    "    #aug\n",
    "    train_x.append(x.permute(1,2,0))\n",
    "    train_y.append(y.permute(1,2,0))\n",
    "    train_x.append(x.permute(2,0,1))\n",
    "    train_y.append(y.permute(2,0,1))\n",
    "path1=\"/root/data/train/kidney_3_sparse\"\n",
    "path2=\"/root/data/train/kidney_3_dense\"\n",
    "paths_y=glob(f\"{path2}/labels/*\")\n",
    "paths_x=[x.replace(\"labels\",\"images\").replace(\"dense\",\"sparse\") for x in paths_y]\n",
    "\n",
    "val_x=load_data(paths_x,is_label=False)\n",
    "print(val_x.shape)\n",
    "val_y=load_data(paths_y,is_label=True)\n",
    "print(val_y.shape)\n",
    "\n",
    "train_dataset=Kaggld_Dataset(train_x,train_y,arg=True)\n",
    "train_dataset = DataLoader(train_dataset, batch_size=CFG.train_batch_size ,num_workers=2, shuffle=True, pin_memory=True)\n",
    "val_dataset=Kaggld_Dataset([val_x],[val_y])\n",
    "val_dataset = DataLoader(val_dataset, batch_size=CFG.valid_batch_size, num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clear tensorboard\n",
    "\n",
    "log_board.clear('train [3P]')\n",
    "log_board.clear('val [3P]')\n",
    "# log_board.clear('train (edge-better2)')\n",
    "# log_board.clear('val (edge-better2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape[1] =2279    x.shape[2]=1303x.shape[1] =1303    x.shape[2]=912\n",
      "\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =2279    x.shape[2]=1303\n",
      "x.shape[1] =2279    x.shape[2]=1303\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =2279    x.shape[2]=1303x.shape[1] =1303    x.shape[2]=912\n",
      "\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912x.shape[1] =912    x.shape[2]=2279\n",
      "\n",
      "x.shape[1] =2279    x.shape[2]=1303\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =2279    x.shape[2]=1303\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =1303    x.shape[2]=912x.shape[1] =2279    x.shape[2]=1303\n",
      "\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =912    x.shape[2]=2279\n",
      "x.shape[1] =1303    x.shape[2]=912\n"
     ]
    }
   ],
   "source": [
    "save_every = 500\n",
    "extra_log_every = 25\n",
    "\n",
    "t_logger = log_board.get_logger('train [3P]')\n",
    "v_logger = log_board.get_logger('val [3P]')\n",
    "\n",
    "model = util.UNet3P(\n",
    "    in_f=1,\n",
    "    layers=[32, 64, 128, 256, 512],\n",
    "    block_depth=4,\n",
    "    connect_depth=24,\n",
    "    conv=util.nn.Conv2DNormed,\n",
    "    pool_fn=nn.MaxPool2d,\n",
    "    resize_kernel=(2,2),\n",
    "    upsample_mode='bilinear',\n",
    "    norm_fn=nn.InstanceNorm2d,\n",
    "    input_noise=None\n",
    "    # dropout=(nn.Dropout2d, 0.2)\n",
    ").cuda()\n",
    "# model.load_state_dict(tc.load('./bin/_tmp_models/unet2.5d_IN_PROGRESS.pt', map_location='cuda:0'))\n",
    "model.train()\n",
    "\n",
    "edge = util.Edge().cuda()\n",
    "loss_fc=DiceLoss()\n",
    "edge_weighted_dice_loss = util.EdgeWeightedDiceLoss(alpha=0.85).cuda()\n",
    "focal_loss = util.BinaryFocalLoss(\n",
    "    alpha=0.8,\n",
    "    gamma=1.5,\n",
    ")\n",
    "\n",
    "#loss_fn=nn.BCEWithLogitsLoss()\n",
    "optimizer=tc.optim.AdamW(model.parameters(),lr=CFG.lr)\n",
    "scaler=tc.cuda.amp.GradScaler()\n",
    "scheduler = tc.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CFG.lr,\n",
    "                                                steps_per_epoch=len(train_dataset), epochs=CFG.epochs+1,\n",
    "                                                pct_start=0.1,)\n",
    "step = 0\n",
    "for _ in range(step):\n",
    "    scheduler.step()\n",
    "for epoch in range(CFG.epochs):\n",
    "    # time=tqdm(range(len(train_dataset)))\n",
    "    # losss=0\n",
    "    # scores=0\n",
    "    for i,(x,y) in enumerate(train_dataset):\n",
    "        step+=1\n",
    "        # prepare data\n",
    "        x=x.cuda().to(tc.float32)\n",
    "        y=y.cuda().to(tc.float32)\n",
    "        x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n",
    "        x=add_noise(x,max_randn_rate=0.5,x_already_normed=True)\n",
    "        y = y.unsqueeze(1)\n",
    "        \n",
    "        # compute prediction\n",
    "        with autocast():\n",
    "            pred=model(x)[-1].sigmoid()\n",
    "            loss= edge_weighted_dice_loss(pred,y)\n",
    "\n",
    "        # loss & gradient step\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        # performance logging\n",
    "        with tc.no_grad():\n",
    "            t_logger.add_scalar('loss', loss.item(), step)\n",
    "            t_logger.add_scalar('lr', optimizer.param_groups[0]['lr'], step)\n",
    "            t_logger.add_scalar('dice', dice_coef(pred.squeeze(1).detach()>0.5,y.squeeze(1)), step)\n",
    "            t_logger.add_scalar('surface-dice', dice_coef(\n",
    "                edge((pred > 0.5).float()),\n",
    "                edge(y)\n",
    "            ), step)\n",
    "            del loss,pred\n",
    "\n",
    "            # score=dice_coef(pred.detach(),y)\n",
    "            # losss=(losss*i+loss.item())/(i+1)\n",
    "            # scores=(scores*i+score)/(i+1)\n",
    "            # time.set_description(f\"epoch:{epoch},loss:{losss:.4f},score:{scores:.4f},lr{optimizer.param_groups[0]['lr']:.4e}\")\n",
    "            # time.update()\n",
    "\n",
    "            if (step + 1) % extra_log_every == 0:\n",
    "                model.eval()\n",
    "                x, y, = next(iter(val_dataset))\n",
    "                x=x.cuda().to(tc.float32)\n",
    "                y=y.cuda().to(tc.float32)\n",
    "                x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n",
    "                y = y.unsqueeze(1)\n",
    "\n",
    "                with autocast():\n",
    "                    pred=model(x)[-1].sigmoid()\n",
    "                    # loss=loss_fc(pred,y)\n",
    "                v_logger.add_scalar('dice', dice_coef(pred.squeeze(1).detach()>0.5,y.squeeze(1)), step)\n",
    "                v_logger.add_scalar('surface-dice', dice_coef(\n",
    "                    edge((pred > 0.5).float()).squeeze(1),\n",
    "                    edge(y).squeeze(1)\n",
    "                ), step)\n",
    "                model.train()\n",
    "            \n",
    "            if (step + 1) % save_every == 0:\n",
    "                tc.save(model.state_dict(), f'./bin/_tmp_models/unet2.5d_IN_PROGRESS.pt')\n",
    "\n",
    "\n",
    "#     time.close()\n",
    "    \n",
    "#     model.eval()\n",
    "#     time=tqdm(range(len(val_dataset)))\n",
    "#     val_losss=0\n",
    "#     val_scores=0\n",
    "#     for i,(x,y) in enumerate(val_dataset):\n",
    "#         x=x.cuda().to(tc.float32)\n",
    "#         y=y.cuda().to(tc.float32)\n",
    "#         x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n",
    "\n",
    "#         with autocast():\n",
    "#             with tc.no_grad():\n",
    "#                 pred=model(x)\n",
    "#                 loss=loss_fc(pred,y)\n",
    "#         score=dice_coef(pred.detach(),y)\n",
    "#         val_losss=(val_losss*i+loss.item())/(i+1)\n",
    "#         val_scores=(val_scores*i+score)/(i+1)\n",
    "#         time.set_description(f\"val-->loss:{val_losss:.4f},score:{val_scores:.4f}\")\n",
    "#         time.update()\n",
    "\n",
    "#     time.close()\n",
    "# #tc.save(model.module.state_dict(),f\"./{CFG.backbone}_{epoch}_loss{losss:.2f}_score{scores:.2f}_val_loss{val_losss:.2f}_val_score{val_scores:.2f}_midd_1024.pt\")\n",
    "\n",
    "# time.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.save(model.state_dict(), f'./bin/_tmp_models/unet2.5d_IN_PROGRESS.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
