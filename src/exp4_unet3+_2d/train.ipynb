{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import util\n",
    "import lutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "log_board = util.diagnostics.LogBoard('log_dir', 6005)\n",
    "log_board.launch()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /train/kidney_1_dense/images from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "E0121 23:44:08.628577 140664648578240 program.py:298] TensorBoard could not bind to port 6005, it was already in use\n",
      "ERROR: TensorBoard could not bind to port 6005, it was already in use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /train/kidney_3_sparse/images from cache\n",
      "Loading /train/kidney_1_dense/labels from cache\n"
     ]
    }
   ],
   "source": [
    "patch_size = 8,256,256\n",
    "train = util.data.SenNet(\n",
    "    patch_size,\n",
    "    guarantee_vessel=0.4,\n",
    "    samples=[\n",
    "        \"/train/kidney_1_dense\",\n",
    "        \"/train/kidney_3_sparse\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "test = util.data.SenNet(\n",
    "    patch_size,\n",
    "    guarantee_vessel=0.4,\n",
    "    samples=[\n",
    "        \"/train/kidney_2\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_board.clear('train')\n",
    "log_board.clear('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 253.94 MiB is free. Process 404489 has 6.90 GiB memory in use. Process 415332 has 16.37 GiB memory in use. Of the allocated memory 15.39 GiB is allocated by PyTorch, and 539.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m],x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m4\u001b[39m]), y[:,:,y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m,:,:]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Compute output\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m p_y \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39msigmoid(logits) \u001b[38;5;28;01mfor\u001b[39;00m logits \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     60\u001b[0m pred_masks \u001b[38;5;241m=\u001b[39m [(p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m p_y]\n\u001b[1;32m     61\u001b[0m masks \u001b[38;5;241m=\u001b[39m [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdeep_masks(y)]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sennet-kaggle/src/exp4_unet3+_2d/lutil.py:250\u001b[0m, in \u001b[0;36mUNet3P.forward\u001b[0;34m(self, x, up_depth)\u001b[0m\n\u001b[1;32m    247\u001b[0m     cross_input \u001b[38;5;241m=\u001b[39m down_agg[:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m([u \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m up_agg])) \u001b[38;5;241m+\u001b[39m [down_agg[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    248\u001b[0m     up_agg\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_blocks[i](cross_input))\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    251\u001b[0m     o(x)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# for o, x in zip(self.out_blocks, [down_agg[-1]] + up_agg)\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_blocks[:up_depth], up_agg)\n\u001b[1;32m    254\u001b[0m ]\n",
      "File \u001b[0;32m~/sennet-kaggle/src/exp4_unet3+_2d/lutil.py:251\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    247\u001b[0m     cross_input \u001b[38;5;241m=\u001b[39m down_agg[:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m([u \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m up_agg])) \u001b[38;5;241m+\u001b[39m [down_agg[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    248\u001b[0m     up_agg\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_blocks[i](cross_input))\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 251\u001b[0m     \u001b[43mo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# for o, x in zip(self.out_blocks, [down_agg[-1]] + up_agg)\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_blocks[:up_depth], up_agg)\n\u001b[1;32m    254\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sennet-kaggle/src/exp4_unet3+_2d/lutil.py:53\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m z \u001b[38;5;241m=\u001b[39m zz\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m---> 53\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m \u001b[38;5;66;03m# Residual connection\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z \u001b[38;5;241m+\u001b[39m zz\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 253.94 MiB is free. Process 404489 has 6.90 GiB memory in use. Process 415332 has 16.37 GiB memory in use. Of the allocated memory 15.39 GiB is allocated by PyTorch, and 539.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "save_every = 500\n",
    "valid_every = 25\n",
    "epochs = 800\n",
    "batch_size = 24\n",
    "t_logger = log_board.get_logger('train')\n",
    "v_logger = log_board.get_logger('val')\n",
    "\n",
    "train_data = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_data = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = lutil.UNet3P(\n",
    "    in_f=8,\n",
    "    layers=[16, 32, 32, 32, 64, 64, 64],\n",
    "    block_depth=4,\n",
    "    connect_depth=6,\n",
    "    conv=util.nn.Conv2DNormed,\n",
    "    pool_fn=nn.MaxPool2d,\n",
    "    resize_kernel=(2,2),\n",
    "    upsample_mode='bilinear',\n",
    "    norm_fn=nn.BatchNorm2d,\n",
    "    dropout=(nn.Dropout2d, 0.1)\n",
    ").to(device)\n",
    "\n",
    "# model = lutil.UNet3P(\n",
    "#     layers=[16, 32, 32, 32, 32, 32, 16],\n",
    "#     block_depth=4,\n",
    "#     connect_depth=4,\n",
    "#     conv=util.nn.Conv3DNormed,\n",
    "#     pool_fn=nn.MaxPool3d,\n",
    "#     resize_kernel=(2,2,1),\n",
    "#     upsample_mode='trilinear',\n",
    "#     norm_fn=nn.BatchNorm3d,\n",
    "#     dropout=(nn.Dropout3d, 0.1)\n",
    "# ).to(device)\n",
    "model.load_state_dict(torch.load('./bin/_tmp_models/unet2.5d_IN_PROGRESS.pt', map_location=device))\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "dice_fn = util.DiceScore().to(device)\n",
    "focal_loss = util.BinaryFocalLoss(\n",
    "    alpha=0.8,\n",
    "    gamma=1.5,\n",
    ")\n",
    "\n",
    "t = time()\n",
    "step = 0\n",
    "for epoch in range(epochs):\n",
    "    for i, (x, y, _) in enumerate(train_data):\n",
    "        step += 1\n",
    "\n",
    "        # Prepare data\n",
    "        aug = util.PatchAugment(no_rotate=True).to(device)\n",
    "        x, y = aug(x.float().to(device)), aug(y.float().to(device))\n",
    "        # x, y = x.squeeze(-1), y.squeeze(-1)\n",
    "        x, y = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4]), y[:,:,y.shape[2]//2,:,:]\n",
    "        # Compute output\n",
    "        p_y = [torch.sigmoid(logits) for logits in model(x)]\n",
    "        pred_masks = [(p > 0.5).float() for p in p_y]\n",
    "        masks = [m for m in model.deep_masks(y)]\n",
    "\n",
    "        # Compute loss\n",
    "        dlayers = [0,1,2,3,4,5]\n",
    "        flyaers = [0,1,2,3,4,5]\n",
    "        dloss = torch.stack([\n",
    "            (1 - dice_fn(p_y[i], masks[i], mode='separate'))\n",
    "            for i in dlayers\n",
    "        ])\n",
    "        dloss[-1] *= 1.1\n",
    "        floss = torch.stack([\n",
    "            focal_loss(p_y[i], masks[i])\n",
    "            for i in flyaers\n",
    "        ])\n",
    "        loss = dloss.sum() + floss.sum()\n",
    "\n",
    "        # Step grad\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Logging & validation\n",
    "        with torch.no_grad():\n",
    "            # log loss\n",
    "            loss_dict = { f'dice_{k}' : dloss[i] for i, k in enumerate(dlayers) }\n",
    "            loss_dict.update({ f'focal_{k}' : floss[i] for i, k in enumerate(flyaers) })\n",
    "            t_logger.add_scalars('loss', loss_dict, step)\n",
    "            \n",
    "            # performance\n",
    "            acc = [torch.eq(pred_masks[i], masks[i]).float().mean() for i in dlayers]\n",
    "            dice = [dice_fn(pred_masks[i], masks[i]) for i in dlayers]\n",
    "            perf_dict = { f'acc_{k}' : acc[i] for i, k in enumerate(dlayers) }\n",
    "            perf_dict.update({ f'dice_{k}' : dice[i] for i, k in enumerate(dlayers) })\n",
    "            t_logger.add_scalars('performance', perf_dict, step)\n",
    "\n",
    "            # stats\n",
    "            for i, (prob, pred, mask) in enumerate(zip(p_y[1:], pred_masks[1:], masks[1:])):\n",
    "                t_logger.add_scalars(f'stats_{i + 1}',{\n",
    "                    'prob_std': prob.std(),\n",
    "                    'prob_mean': prob.mean(),\n",
    "                    'pred_std': pred.std(),\n",
    "                    'pred_mean': pred.mean(),\n",
    "                    'mask_std': mask.std(),\n",
    "                    'mask_mean': mask.mean(),\n",
    "                }, step)\n",
    "\n",
    "            # Validation logging\n",
    "            if (step + 1) % valid_every == 0:\n",
    "                model.eval()\n",
    "                x, y, _ = next(iter(valid_data))\n",
    "                x, y = x.float().to(device), y.float().to(device)\n",
    "                x, y = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4]), y[:,:,y.shape[2]//2,:,:]\n",
    "                # x, y = x.squeeze(-1), y.squeeze(-1)\n",
    "\n",
    "                # Compute output\n",
    "                p_y = [torch.sigmoid(logits) for logits in model(x)]\n",
    "                pred_masks = [(p > 0.5).float() for p in p_y]\n",
    "                masks = [m for m in model.deep_masks(y)]\n",
    "\n",
    "                acc = [torch.eq(pred_masks[i], masks[i]).float().mean() for i in dlayers]\n",
    "                dice = [dice_fn(pred_masks[i], masks[i]) for i in dlayers]\n",
    "                perf_dict = { f'acc_{k}' : acc[i] for i, k in enumerate(dlayers) }\n",
    "                perf_dict.update({ f'dice_{k}' : dice[i] for i, k in enumerate(dlayers) })\n",
    "                v_logger.add_scalars('performance', perf_dict, step)\n",
    "\n",
    "                v_logger.add_image('masks', util.mask_plots2d(x[:,x.shape[1]//2,:,:].unsqueeze(1), masks, pred_masks), step, dataformats='HWC')\n",
    "                t_logger.add_scalar('time', time() - t, step)\n",
    "                t = time()\n",
    "                model.train()\n",
    "                \n",
    "            # Save model\n",
    "            if (step + 1) % save_every == 0:\n",
    "                torch.save(model.state_dict(), f'./bin/_tmp_models/unet2.5d_IN_PROGRESS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('./bin/models', exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), f'./bin/models/unet2d_3dinput_450epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 0]),\n",
       " tensor([64,  8, 64]),\n",
       " tensor([2, 0, 1]),\n",
       " torch.Size([1, 64, 8, 64]),\n",
       " torch.Size([1, 64, 64, 8]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_size = torch.tensor([64,64,8])\n",
    "k_perm = torch.randperm(3)\n",
    "patch_size = p_size[k_perm]\n",
    "\n",
    "x = torch.randn(1,*p_size[k_perm])\n",
    "# x = x.permute(0, *k_perm+1)\n",
    "k_perm, patch_size, torch.argsort(k_perm), x.shape, x.permute(0, *torch.argsort(k_perm)+1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 64, 64, 8]), torch.Size([32, 1, 64, 64, 8]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_data = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_data = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y, _ = next(iter(train_data))\n",
    "aug = util.PatchAugment(no_rotate=True).to(device)\n",
    "x, y = aug(x.float().to(device)), aug(y.float().to(device))\n",
    "\n",
    "x.shape, y.shape\n",
    "\n",
    "# util.Display(x[0].squeeze().cpu().transpose(0,2), y[0].squeeze().cpu().transpose(0,2) )()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lutil.UNet3P(\n",
    "#     layers=[16, 32, 64, 128, 16],\n",
    "#     conv=util.nn.Conv3DNormed,\n",
    "#     block_depth=4,\n",
    "#     connect_depth=8,\n",
    "#     norm_fn=nn.BatchNorm3d,\n",
    "#     resize_kernel=(2,2,1),\n",
    "#     dropout=(nn.Dropout3d, 0.1)\n",
    "# ).to(device)\n",
    "\n",
    "model = lutil.UNet3P(\n",
    "    layers=[16, 32, 64, 128, 16],\n",
    "    block_depth=4,\n",
    "    connect_depth=8,\n",
    "\n",
    "    conv=nn.Conv2d,\n",
    "    pool_fn=nn.MaxPool2d,\n",
    "    resize_kernel=(2,2),\n",
    "    upsample_mode='bilinear',\n",
    "    norm_fn=nn.BatchNorm2d,\n",
    "    dropout=(nn.Dropout2d, 0.1)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64, 64,  8])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "patch_size = (64,64,8)\n",
    "patch_size = torch.tensor(patch_size)\n",
    "patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64,  8, 64])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm = torch.randperm(len(patch_size))\n",
    "patch_size = patch_size[perm]\n",
    "patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), tensor([64, 64,  8]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_perm = np.random.permutation(3)\n",
    "k_perm\n",
    "k_perm, p_size[k_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(8), tensor(64), tensor(64)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*p_size[k_perm]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_perm = torch.randperm(3)\n",
    "k_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 0]), tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(k_perm), k_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 2]),\n",
       " tensor([1, 0, 2]),\n",
       " torch.Size([1, 64, 64, 8]),\n",
       " torch.Size([1, 64, 64, 8]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 2, 1]), tensor([0, 2, 1]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_perm,torch.argsort(k_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([1, 1, 16, 16]),\n",
       "  torch.Size([1, 1, 32, 32]),\n",
       "  torch.Size([1, 1, 64, 64]),\n",
       "  torch.Size([1, 1, 128, 128])],\n",
       " [torch.Size([1, 1, 16, 16]),\n",
       "  torch.Size([1, 1, 32, 32]),\n",
       "  torch.Size([1, 1, 64, 64]),\n",
       "  torch.Size([1, 1, 128, 128])])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = torch.randn(1, 1, 128, 128).to(device), torch.randn(1, 1, 128, 128).to(device)\n",
    "\n",
    "p_y = [F.sigmoid(x) for x in model(x)]\n",
    "masks = model.deep_masks(y)\n",
    "\n",
    "[p.shape for p in p_y], [m.shape for m in masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool3d(kernel_size=tensor([16, 16,  1]), stride=tensor([16, 16,  1]), padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MaxPool3d(torch.tensor([2,2,1]) ** 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 64, 64).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
