{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_743576/1378842735.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from glob import glob\n",
    "import tifffile as tiff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "log_board = util.diagnostics.LogBoard('log_dir', 6005)\n",
    "log_board.launch()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_dir = '/root/data'\n",
    "model_pth = './bin/models/edge_model_trained5.pt'\n",
    "scan_folders = [\n",
    "    f'{data_dir}/train/kidney_2/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    pixel = mask.flatten()\n",
    "    pixel = np.concatenate([[0], pixel, [0]])\n",
    "    run = np.where(pixel[1:] != pixel[:-1])[0] + 1\n",
    "    run[1::2] -= run[::2]\n",
    "    rle = ' '.join(str(r) for r in run)\n",
    "    if rle == '':\n",
    "        rle = '1 0'\n",
    "    return rle\n",
    "\n",
    "def id_from_pth(pth: str):\n",
    "    parts = pth.split(\"/\")[-3:]\n",
    "    parts.pop(1)\n",
    "    return \"_\".join(parts)[:-4]\n",
    "\n",
    "def proprocess(_scan: np.ndarray):\n",
    "    scan = _scan.astype(np.float32)\n",
    "    smin, smax = np.min(scan), np.max(scan)\n",
    "    scan = (255 * (scan - smin) / (smax - smin)).astype(np.uint8)\n",
    "    scan = 255 - scan\n",
    "    clahe = cv2.createCLAHE(clipLimit=40.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(scan)\n",
    "\n",
    "def load_slice(pth, preprocess_fn):\n",
    "    return torch.tensor(\n",
    "        proprocess(tiff.imread(pth))\n",
    "    )\n",
    "\n",
    "\n",
    "def fill_outline(outline: torch.Tensor) -> torch.Tensor:\n",
    "    outline = (outline.squeeze(0) * 255).numpy().astype(np.uint8)\n",
    "\n",
    "    contours, _ = cv2.findContours(outline, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filled_image = np.zeros_like(outline)\n",
    "    cv2.drawContours(filled_image, contours, -1, (255), thickness=cv2.FILLED)\n",
    "    return torch.Tensor(filled_image).bool().unsqueeze(0)\n",
    "\n",
    "class Patch2p5D(nn.Module):\n",
    "    def __init__(self, model: util.UNet3P):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4])\n",
    "        return self.model(z)[-1]\n",
    "\n",
    "\n",
    "class ScanInference2p5D(nn.Module):\n",
    "    def __init__(self, patch_fn, batch_size: int, quick: bool = False):\n",
    "        super().__init__()\n",
    "        self.patch_fn = patch_fn\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.patch_size = (1, 256, 256)\n",
    "        self.perms = torch.Tensor([\n",
    "            (0, 1, 2),\n",
    "            (1, 0, 2),\n",
    "            (2, 1, 0)\n",
    "\n",
    "            # (0, 2, 1),\n",
    "            # (1, 0, 2),\n",
    "            # (1, 2, 0),\n",
    "            # (2, 0, 1),\n",
    "            #(2, 1, 0), 5 perms to make self.pass_max a nice number\n",
    "        ]).int()\n",
    "        if quick:\n",
    "            self.perms = self.perms[:1]\n",
    "        \n",
    "        self.register_buffer('pass_max', torch.tensor(255 / (len(self.perms) * 4)))\n",
    "    \n",
    "    def _forward(self, scan: torch.Tensor, device: torch.device = 'cpu') -> torch.Tensor:\n",
    "        agg_pred = torch.zeros_like(scan, dtype=torch.uint8)\n",
    "        scan_loader = DataLoader(\n",
    "            util.SweepCube(scan, self.patch_size, stride=(1, self.patch_size[1]//2, self.patch_size[2]//2)),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        for x, positions in scan_loader:\n",
    "            x = x.to(device).float()\n",
    "            pred = F.sigmoid(self.patch_fn(x)).cpu()\n",
    "            for p, pos in zip(pred, positions):\n",
    "                agg_pred[\n",
    "                    :,\n",
    "                    pos[0] + self.patch_size[0]//2,\n",
    "                    pos[1]:pos[1] + self.patch_size[1],\n",
    "                    pos[2]:pos[2] + self.patch_size[2],\n",
    "                ] += ((p.squeeze(1) * self.pass_max).round()).byte()#(p.squeeze(1) * 255).int\n",
    "        \n",
    "        return agg_pred\n",
    "\n",
    "    def forward(self, scan: torch.Tensor, device: torch.device = 'cpu') -> torch.Tensor:\n",
    "        agg_pred = torch.zeros_like(scan, dtype=torch.uint8)\n",
    "\n",
    "        for perm in self.perms:\n",
    "            out = self._forward(scan.permute(0, *perm+1), device).permute(0, *torch.argsort(perm)+1)\n",
    "            agg_pred += out\n",
    "            del out\n",
    "        return agg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /train/kidney_2/images from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.1 at http://localhost:6005/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /train/kidney_2/labels from cache\n"
     ]
    }
   ],
   "source": [
    "patch_size = 1,256,256\n",
    "train_data = util.data.SenNet(\n",
    "    patch_size,\n",
    "    guarantee_vessel=0.5,\n",
    "    samples=[\n",
    "        \"/train/kidney_2\",\n",
    "        # \"/train/kidney_3_dense\",\n",
    "        # \"/train/kidney_3_sparse\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = Patch2p5D(\n",
    "    util.UNet3P(\n",
    "        in_f=1,\n",
    "        layers=[32, 64, 128, 256, 512],\n",
    "        block_depth=4,\n",
    "        connect_depth=24,\n",
    "        conv=util.nn.Conv2DNormed,\n",
    "        pool_fn=nn.MaxPool2d,\n",
    "        resize_kernel=(2,2),\n",
    "        upsample_mode='bilinear',\n",
    "        norm_fn=nn.InstanceNorm2d,\n",
    "    )\n",
    ").to(device)\n",
    "patcher.model.load_state_dict(torch.load(model_pth, map_location=device))\n",
    "patcher.requires_grad_(False)\n",
    "patcher.eval()\n",
    "\n",
    "inference = ScanInference2p5D(\n",
    "    patcher,\n",
    "    batch_size=64,\n",
    "    quick=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scan /root/data/train/kidney_2/\n",
      "doing inference...\n",
      "aggregating slice rle encodings...\n",
      "saving aggregate dataframe of rle encodings...\n"
     ]
    }
   ],
   "source": [
    "th = 0.5\n",
    "threshold = 50#int(255 * th)\n",
    "\n",
    "submission_list = []\n",
    "for scan_fn in scan_folders:\n",
    "    slices, ids = [], []\n",
    "    print(f\"loading scan {scan_fn}\")\n",
    "    for pth in sorted(glob(scan_fn + \"images/*.tif\")):\n",
    "        slices.append(load_slice(pth, proprocess))\n",
    "        ids.append(id_from_pth(pth))\n",
    "    if len(slices) == 0:\n",
    "        continue\n",
    "    scan = torch.stack(slices).unsqueeze(0)\n",
    "\n",
    "    print(\"doing inference...\")\n",
    "    pmask = inference(scan, device).squeeze(0)\n",
    "\n",
    "    print(\"aggregating slice rle encodings...\")\n",
    "    for id, smask in zip(ids, pmask):\n",
    "        submission_list.append(\n",
    "            pd.DataFrame(data={\n",
    "                'id'  : id,\n",
    "                'rle' : rle_encode((smask > threshold).numpy()),\n",
    "            },index=[0])\n",
    "        )\n",
    "    # del scan\n",
    "    # del pmask\n",
    "    # del slices\n",
    "    # del ids\n",
    "\n",
    "print(\"saving aggregate dataframe of rle encodings...\")\n",
    "submission_df = pd.concat(submission_list)\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pmask, 'pmask.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fill_all_axis(seg: torch.Tensor) -> torch.Tensor:\n",
    "    for i in range(seg.shape[0]):\n",
    "        seg[i] = fill_outline(seg[i])\n",
    "    for i in range(seg.shape[1]):\n",
    "        seg[:,i] = fill_outline(seg[:,i])\n",
    "    for i in range(seg.shape[2]):\n",
    "        seg[:,:,i] = fill_outline(seg[:,:,i])\n",
    "    return seg\n",
    "\n",
    "def fill_all_axis(seg: torch.Tensor, iters: int) -> torch.Tensor:\n",
    "    for i in range(iters):\n",
    "        _fill_all_axis(seg)\n",
    "    return seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = torch.load('pmask.pt') > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = fill_all_axis(seg, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seg.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmask = seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp w/ threshold\n",
    "submission_list = []\n",
    "for id, smask in zip(ids, seg):\n",
    "    submission_list.append(\n",
    "        pd.DataFrame(data={\n",
    "            'id'  : id,\n",
    "            'rle' : rle_encode(((smask)).numpy()),\n",
    "        },index=[0])\n",
    "    )\n",
    "submission_df = pd.concat(submission_list)\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a629805ae3d4218850ff4b3cf77f848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=2216), Output()), _dom_classes=('widget-interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "util.Display.view([\n",
    "    util.Display(pmask.unsqueeze(0) > 50),\n",
    "    util.Display(train_data.labels[0]),\n",
    "    # util.Display(train_data.scans[0]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('submission.csv')\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# slices = []\n",
    "# for row in submission_df.itertuples():\n",
    "#     slices.append()\n",
    "pmask2 = torch.stack(\n",
    "    [torch.tensor(rle_decode(row.rle, (1303, 912)))\n",
    "    for row in submission_df.itertuples()]\n",
    ").unsqueeze(0).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2279, 1303, 912])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmask2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fba7cc5adde46cea0420486d8aee70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=2278), Output()), _dom_classes=('widget-interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "util.Display.view([\n",
    "    util.Display(pmask2),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
