{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "log_board = util.diagnostics.LogBoard('log_dir', 6005)\n",
    "log_board.launch()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "E0131 02:16:44.540774 140487363224192 program.py:298] TensorBoard could not bind to port 6005, it was already in use\n",
      "ERROR: TensorBoard could not bind to port 6005, it was already in use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /train/kidney_2/images from cache\n",
      "Loading /train/kidney_2/labels from cache\n"
     ]
    }
   ],
   "source": [
    "patch_size = 1,256,256\n",
    "train = util.data.SenNet(\n",
    "    patch_size,\n",
    "    guarantee_vessel=0.5,\n",
    "    data= [\n",
    "        [\n",
    "            util.data.kidney_from_cache(\"kidney_1_dense\", \"images\"),\n",
    "            util.data.kidney_from_cache(\"kidney_3_dense\", \"images\")\n",
    "        ], [\n",
    "            util.data.kidney_1_fixed(),\n",
    "            util.data.kidney_from_cache(\"kidney_3_dense\", \"labels\")\n",
    "        ]\n",
    "    ]\n",
    "    # samples=[\n",
    "        # \"/train/kidney_1_dense\",\n",
    "        # \"/train/kidney_3_dense\",\n",
    "        # \"/train/kidney_3_sparse\"\n",
    "    # ]\n",
    ")\n",
    "\n",
    "test = util.data.SenNet(\n",
    "    patch_size,\n",
    "    guarantee_vessel=0.5,\n",
    "    samples=[\n",
    "        \"/train/kidney_2\"\n",
    "        # \"/train/kidney_3_sparse\"\n",
    "    ]\n",
    ")\n",
    "test.scans[0] = test.scans[0][:, 900:, :, :]\n",
    "test.labels[0] = test.labels[0][:, 900:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_board.clear('train')\n",
    "log_board.clear('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as VF\n",
    "import albumentations as A\n",
    "import random\n",
    "\n",
    "# augment\n",
    "\n",
    "\n",
    "augment_just_scan = T.Compose([\n",
    "    T.RandomInvert(p=0.5),\n",
    "    T.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "    # T.ColorJitter(brightness=0.15, contrast=0.15)\n",
    "])\n",
    "\n",
    "def augment(x: torch.Tensor, y: torch.Tensor, do_just_x: bool = True) -> (torch.Tensor, torch.Tensor):\n",
    "    _x, _y = x, y\n",
    "    if do_just_x:\n",
    "        _x = augment_just_scan(x)\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        seed = random.randint(0, 2**32)\n",
    "        elastic_transform = T.ElasticTransform(\n",
    "            alpha= random.random() * 80.,\n",
    "            sigma= 15.\n",
    "        )\n",
    "\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        _x = elastic_transform(x)\n",
    "\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        _y = elastic_transform(y)\n",
    "\n",
    "    return _x, _y\n",
    "    # return augment_just_scan(augment(x)), augment(y)\n",
    "\n",
    "    # train_aug_list = [\n",
    "    #     A.Rotate(limit=270, p= 0.5),\n",
    "    #     A.RandomScale(scale_limit=(0.8,1.25),interpolation=cv2.INTER_CUBIC,p=p_augm),\n",
    "    #     A.RandomCrop(input_size, input_size,p=1),\n",
    "    #     A.RandomGamma(p=p_augm*2/3),\n",
    "    #     A.RandomBrightnessContrast(p=p_augm,),\n",
    "    #     A.GaussianBlur(p=p_augm),\n",
    "    #     A.MotionBlur(p=p_augm),\n",
    "    #     A.GridDistortion(num_steps=5, distort_limit=0.3, p=p_augm),\n",
    "    #     ToTensorV2(transpose_mask=True),\n",
    "    # ]\n",
    "    # train_aug = A.Compose(train_aug_list)\n",
    "    # valid_aug_list = [\n",
    "    #     ToTensorV2(transpose_mask=True),\n",
    "    # ]\n",
    "    # valid_aug = A.Compose(valid_aug_list)\n",
    "\n",
    "# def augment(x: torch.Tensor) -> torch.Tensor:\n",
    "#     # random dimming\n",
    "#     _x = x * torch.rand(1, device=x.device) * 0.8 + 0.5\n",
    "#     # _x = _x ** (torch.rand(1, device=x.device) * 2 + 0.5)\n",
    "#     return _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m dloss\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m floss\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;66;03m#+ divergence_loss      # total loss\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# step grad\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "save_every = 500\n",
    "extra_log_every = 25\n",
    "epochs = 800\n",
    "batch_size = 8\n",
    "t_logger = log_board.get_logger('train')\n",
    "v_logger = log_board.get_logger('val')\n",
    "\n",
    "train_data = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_data = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "edge = util.Edge().to(device)\n",
    "\n",
    "# TODO: use this layout next time (small down size, large cross size)\n",
    "model = util.UNet3P(\n",
    "    in_f=1,\n",
    "    layers=[32, 64, 128, 256, 512],\n",
    "    block_depth=4,\n",
    "    connect_depth=24,\n",
    "    conv=util.nn.Conv2DNormed,\n",
    "    pool_fn=nn.MaxPool2d,\n",
    "    resize_kernel=(2,2),\n",
    "    upsample_mode='bilinear',\n",
    "    norm_fn=nn.InstanceNorm2d,\n",
    "    # dropout=(nn.Dropout2d, 0.2)\n",
    ").to(device)\n",
    "# model = util.UNet3P(\n",
    "#     in_f=3,\n",
    "#     layers=[32, 64, 64, 128, 128, 128, 128],\n",
    "#     block_depth=6,\n",
    "#     connect_depth=8,\n",
    "#     conv=util.nn.Conv2DNormed,\n",
    "#     pool_fn=nn.MaxPool2d,\n",
    "#     resize_kernel=(2,2),\n",
    "#     upsample_mode='bilinear',\n",
    "#     norm_fn=nn.BatchNorm2d,\n",
    "#     frozen_norm=True,\n",
    "#     dropout=(nn.Dropout2d, 0.1)\n",
    "# ).to(device)\n",
    "# model.load_state_dict(torch.load('./bin/models/pretrain_model_base.pt', map_location=device))\n",
    "model.train()\n",
    "\n",
    "# pull out trainable parameters\n",
    "# for module in model.modules():\n",
    "#     if isinstance(module, nn.BatchNorm2d):\n",
    "#         for param in module.parameters():\n",
    "#             param.requires_grad = False\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "init_weights = [p.detach().clone() for p in trainable_params]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=0.001)\n",
    "dice_fn = util.DiceScore().to(device)\n",
    "edge_weighted_dice_loss = util.EdgeWeightedDiceLoss(alpha=0.95).to(device)\n",
    "focal_loss = util.BinaryFocalLoss(\n",
    "    alpha=0.8,\n",
    "    gamma=1.5,\n",
    ")\n",
    "\n",
    "t = time()\n",
    "step = 0\n",
    "for epoch in range(epochs):\n",
    "    for i, (x, y, _) in enumerate(train_data):\n",
    "        step += 1\n",
    "\n",
    "        # prepare data\n",
    "        aug = util.PatchAugment(no_rotate=True).to(device)\n",
    "        x, y = aug(x.float().to(device)), aug(y.float().to(device))\n",
    "        x, y = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4]), y[:,:,y.shape[2]//2,:,:]\n",
    "        x, y = augment(x, y)\n",
    "\n",
    "        # compute output\n",
    "        p_y = [torch.sigmoid(logits) for logits in model(x)]\n",
    "        pred_masks = [(p > 0.5).float().detach() for p in p_y]\n",
    "        masks = [m for m in model.deep_masks(y)]\n",
    "\n",
    "        # compute loss\n",
    "        dlayers = [0,1,2,3]\n",
    "        flayers = [0,1,2,3]\n",
    "        dloss = torch.stack([                                   # dice loss\n",
    "            1 - dice_fn(p_y[i], masks[i]) if i not in [4,5]\n",
    "            else edge_weighted_dice_loss(p_y[i], masks[i])\n",
    "            for i in dlayers\n",
    "        ])\n",
    "        dloss[-1] *= 1.1\n",
    "        floss = torch.stack([                                   # focal loss\n",
    "            focal_loss(p_y[i], masks[i])\n",
    "            for i in flayers\n",
    "        ])\n",
    "        # penalty_weight = 0.001\n",
    "        # divergence_loss = torch.stack([                         # KL divergence loss\n",
    "        #     torch.norm(p - init, 2)\n",
    "        #     for p, init in zip(trainable_params, init_weights)\n",
    "        # ]).mean() * penalty_weight\n",
    "\n",
    "        loss = dloss.sum() + floss.sum() #+ divergence_loss      # total loss\n",
    "\n",
    "        # step grad\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Logging & validation\n",
    "        with torch.no_grad():\n",
    "            # log loss\n",
    "            loss_dict = { f'dice_{k}' : dloss[i] for i, k in enumerate(dlayers) }\n",
    "            loss_dict.update({ f'focal_{k}' : floss[i] for i, k in enumerate(flayers) })\n",
    "            # loss_dict['divergence'] = divergence_loss\n",
    "            t_logger.add_scalars('loss', loss_dict, step)\n",
    "            \n",
    "            # peformance logging\n",
    "            t_logger.add_scalars('acc', { f'layer_{i}' : torch.eq(pred_masks[i], masks[i]).float().mean() for i in dlayers }, step)\n",
    "            t_logger.add_scalars('dice', { f'layer_{i}' : dice_fn(pred_masks[i], masks[i]) for i in dlayers }, step)\n",
    "            t_logger.add_scalars('surface dice', { f'layer_{i}' : dice_fn(edge(pred_masks[i]), edge(masks[i])) for i in dlayers[-2:] }, step)\n",
    "\n",
    "            # stats logging\n",
    "            for i, (prob, pred, mask) in enumerate(zip(p_y[1:], pred_masks[1:], masks[1:])):\n",
    "                t_logger.add_scalars(f'stats_{i + 1}',{\n",
    "                    'prob_std': prob.std(),\n",
    "                    'prob_mean': prob.mean(),\n",
    "                    'pred_std': pred.std(),\n",
    "                    'pred_mean': pred.mean(),\n",
    "                    'mask_std': mask.std(),\n",
    "                    'mask_mean': mask.mean(),\n",
    "                }, step)\n",
    "\n",
    "            # validation logging\n",
    "            if (step + 1) % extra_log_every == 0:\n",
    "                model.eval()\n",
    "\n",
    "                # prepare data\n",
    "                x, y, _ = next(iter(valid_data))\n",
    "\n",
    "                x, y = x.float().to(device), y.float().to(device)\n",
    "                x, y = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4]), y[:,:,y.shape[2]//2,:,:]\n",
    "                # x, y = augment(x, y, False)\n",
    "                # x, y = x.float().to(device), y.float().to(device)\n",
    "                # x, y = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4]), y[:,:,y.shape[2]//2,:,:]\n",
    "                # x = augment(x) # extra color augment on x\n",
    "\n",
    "                # compute output\n",
    "                p_y = [torch.sigmoid(logits) for logits in model(x)]\n",
    "                pred_masks = [(p > 0.5).float() for p in p_y]\n",
    "                masks = [m for m in model.deep_masks(y)]\n",
    "\n",
    "                # peformance logging\n",
    "                v_logger.add_scalars('acc', { f'layer_{i}' : torch.eq(pred_masks[i], masks[i]).float().mean() for i in dlayers }, step)\n",
    "                v_logger.add_scalars('dice', { f'layer_{i}' : dice_fn(pred_masks[i], masks[i]) for i in dlayers }, step)\n",
    "                v_logger.add_scalars('surface dice', { f'layer_{i}' : dice_fn(edge(pred_masks[i]), edge(masks[i])) for i in dlayers[-2:] }, step)\n",
    "\n",
    "                # other logging\n",
    "                v_logger.add_image('masks', util.mask_plots(x[:,x.shape[1]//2,:,:].unsqueeze(1), masks, pred_masks), step, dataformats='HWC')\n",
    "                t_logger.add_scalar('time', time() - t, step)\n",
    "                t = time()\n",
    "                model.train()\n",
    "                \n",
    "            # Save model\n",
    "            if (step + 1) % save_every == 0:\n",
    "                torch.save(model.state_dict(), f'./bin/_tmp_models/unet2.5d_IN_PROGRESS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_board.clear('val-(with aug)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m mask \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# peformance logging\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mv_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m v_logger\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdice\u001b[39m\u001b[38;5;124m'\u001b[39m, dice_fn(pred_mask, mask), step)\n\u001b[1;32m     54\u001b[0m v_logger\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface dice\u001b[39m\u001b[38;5;124m'\u001b[39m, dice_fn(edge(pred_mask), edge(mask)), step)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py:384\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m workspace\n\u001b[1;32m    382\u001b[0m     scalar_value \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mFetchBlob(scalar_value)\n\u001b[0;32m--> 384\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mscalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:333\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscalar\u001b[39m(name, tensor, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, new_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, double_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Outputs a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmake_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    335\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    336\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/tensorboard/_convert_np.py:23\u001b[0m, in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_prepare_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but numpy array, torch tensor, or caffe2 blob name are expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/tensorboard/_convert_np.py:30\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_pytorch\u001b[39m(x):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Test just validation\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "extra_log_every = 50\n",
    "batch_size = 32\n",
    "v_logger = log_board.get_logger('val ( > 0.1)')\n",
    "\n",
    "valid_data = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "edge = util.Edge().to(device)\n",
    "\n",
    "model_pth = './bin/models/edge_model_trained4.pt'\n",
    "model = util.UNet3P(\n",
    "    in_f=1,\n",
    "    layers=[32, 64, 128, 256, 512],\n",
    "    block_depth=4,\n",
    "    connect_depth=24,\n",
    "    conv=util.nn.Conv2DNormed,\n",
    "    pool_fn=nn.MaxPool2d,\n",
    "    resize_kernel=(2,2),\n",
    "    upsample_mode='bilinear',\n",
    "    norm_fn=nn.InstanceNorm2d,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_pth, map_location=device))\n",
    "model.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "dice_fn = util.DiceScore().to(device)\n",
    "edge_weighted_dice_loss = util.EdgeWeightedDiceLoss(alpha=0.95).to(device)\n",
    "focal_loss = util.BinaryFocalLoss(\n",
    "    alpha=0.8,\n",
    "    gamma=1.5,\n",
    ")\n",
    "\n",
    "t = time()\n",
    "step = 0\n",
    "for step in range(100000):\n",
    "    # Logging & validation\n",
    "    with torch.no_grad():\n",
    "        # prepare data\n",
    "        x, y, _ = next(iter(valid_data))\n",
    "        x, y = x.float().to(device), y.float().to(device)\n",
    "        x, y = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4]), y[:,:,y.shape[2]//2,:,:]\n",
    "        x = augment(x) # extra color augment on x\n",
    "\n",
    "        # compute output\n",
    "        p_y = torch.sigmoid(model(x)[-1])\n",
    "        pred_mask = (p_y > 0.1).float()\n",
    "        mask = y\n",
    "\n",
    "        # peformance logging\n",
    "        v_logger.add_scalar('acc', torch.eq(pred_mask, mask).float().mean(), step)\n",
    "        v_logger.add_scalar('dice', dice_fn(pred_mask, mask), step)\n",
    "        v_logger.add_scalar('surface dice', dice_fn(edge(pred_mask), edge(mask)), step)\n",
    "\n",
    "        # other logging\n",
    "        if (step + 1) % extra_log_every == 0:\n",
    "            v_logger.add_image('masks', util.mask_plots(x[:,x.shape[1]//2,:,:].unsqueeze(1), masks, pred_masks), step, dataformats='HWC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
